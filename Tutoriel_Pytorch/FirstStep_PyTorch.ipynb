{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans PyTorch, tout est un tenseur. Débutons par créer un tenseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([[[[2.1715e-18, 1.8888e+31, 4.7414e+16],\n",
      "          [6.4094e-10, 8.1424e-13, 1.4585e-19],\n",
      "          [6.4094e-10, 8.1424e-13, 1.4585e-19]],\n",
      "\n",
      "         [[6.4094e-10, 8.1424e-13, 1.4585e-19],\n",
      "          [6.4094e-10, 8.1424e-13, 1.4585e-19],\n",
      "          [6.4094e-10, 8.1424e-13, 1.4585e-19]]],\n",
      "\n",
      "\n",
      "        [[[1.0663e-08, 1.3028e-11, 1.9388e+17],\n",
      "          [1.3563e-19, 1.3563e-19, 1.0383e-05],\n",
      "          [5.2599e+22, 2.6033e-12, 1.0383e-05]],\n",
      "\n",
      "         [[5.2012e+22, 2.5606e-12, 2.5350e-09],\n",
      "          [5.4365e+22, 2.6175e-12, 1.0140e-08],\n",
      "          [5.4373e+22, 8.3401e+17, 3.2304e-18]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(1)  #Prend son size en argument\n",
    "y = torch.empty(3)  #Vecteur 1D avec 3 élèments\n",
    "print(y)\n",
    "z = torch.empty(2, 2, 3, 3)  #Vecteur 4D\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2425, 0.9404],\n",
      "        [0.5248, 0.5002]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2) #Prend size en argument -> Retourne tenseur random\n",
    "print(x)\n",
    "x = torch.zeros(2, 2)  #Équivalent à numpy\n",
    "x = torch.ones(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De base, les tenseurs sont de type *torch.float32*. On peut toutefois les convertirs avec *dtype* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float16\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, dtype=torch.double)\n",
    "print(x.dtype)\n",
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convertir une liste en tenseur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.5000,  0.1000, 29.0000])\n"
     ]
    }
   ],
   "source": [
    "liste = [2.5, 0.1, 29]\n",
    "x = torch.tensor(liste)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition/soustraction/multiplication/division avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5690, 1.3868],\n",
      "        [0.3750, 1.5473]])\n",
      "tensor([[1.5690, 1.3868],\n",
      "        [0.3750, 1.5473]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.5690, 1.3868],\n",
       "        [0.3750, 1.5473]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2,2) \n",
    "print(x + y)\n",
    "print(torch.add(x,y))  # Exactement pareil à x+y\n",
    "y.add_(x)  # On vient redéfinir y en ajoutant x. Les opérations avec _ redéfinisse la variable\n",
    "# x * y ou torch.mul pour multip | x / y ou torch.div pour division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6783, 0.7785, 0.6247],\n",
      "        [0.2020, 0.9870, 0.0875],\n",
      "        [0.4326, 0.5474, 0.0277],\n",
      "        [0.3054, 0.9637, 0.7080],\n",
      "        [0.2614, 0.9378, 0.9006]])\n",
      "tensor([0.6783, 0.2020, 0.4326, 0.3054, 0.2614])\n",
      "tensor([0.2020, 0.9870, 0.0875])\n",
      "tensor(0.9870)\n",
      "0.9870083928108215\n"
     ]
    }
   ],
   "source": [
    "#Slicing\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:,0])  #x[ligne:colonne] -> x[:,0] = On veut toute la colonne 0\n",
    "print(x[1,:]) #Ligne 1 avec toutes les colonnes\n",
    "print(x[1,1])  #Élèment à la position (1,1)\n",
    "print(x[1,1].item())  # Permet d'avoir la valeur exacte. Fonctionne pour 1 tenseur à 1 terme seulement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape un tenseur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3817, 0.2854],\n",
      "          [0.6098, 0.8032]],\n",
      "\n",
      "         [[0.4815, 0.9390],\n",
      "          [0.7699, 0.6433]]],\n",
      "\n",
      "\n",
      "        [[[0.3229, 0.6010],\n",
      "          [0.2862, 0.5417]],\n",
      "\n",
      "         [[0.0583, 0.7235],\n",
      "          [0.9591, 0.9337]]]])\n",
      "tensor([[0.3817, 0.2854, 0.6098, 0.8032, 0.4815, 0.9390, 0.7699, 0.6433],\n",
      "        [0.3229, 0.6010, 0.2862, 0.5417, 0.0583, 0.7235, 0.9591, 0.9337]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)  # Total 16 valeurs\n",
    "y = x.view(16)  # Remet les 16 valeurs en 1 ligne\n",
    "y = x.view(2,2,2,2)  # Remet les 16 valeurs en 2 lignes\n",
    "z = x.view(-1, 8)  # Trouve la bonne grosseur du tenseur avec 8 termes/lignes\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertir Torch en Numpy et vice-versa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()  # Devient numpy\n",
    "print(type(b))\n",
    "\n",
    "a.add_(1)\n",
    "print(a)  # Attention : On a modifié a, mais b est quand même modifié car\n",
    "print(b)  # il partage la même place dans le gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "a += 1\n",
    "print(a)  # Attention : b a aussi été changé\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nop\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y  # Performe sur le GPU\n",
    "    # On ne peut pas faire de z.numpy() car numpy ne fonctionne pas sur GPU\n",
    "    z = z.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutoriel #2 : Autograd**\n",
    "\n",
    "Calculer des gradients dans PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2926,  2.2143, -0.7689], requires_grad=True)\n",
      "tensor([2.2926, 4.2143, 1.2311], grad_fn=<AddBackward0>)\n",
      "tensor([10.5124, 35.5209,  3.0312], grad_fn=<MulBackward0>)\n",
      "tensor(16.3548, grad_fn=<MeanBackward0>)\n",
      "tensor([3.0569, 5.6191, 1.6415])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)  # rand -> Retourne tenseur avec nombres aléatoires avec distribution normale. sigma=1 mu = 0\n",
    "print(x)                                #requires_grad informe que nous voulons calculer le gradient\n",
    "\n",
    "y = x + 2  # Voir dessin (1)\n",
    "print(y)  #grad_fn car output, add car addition et Backward car backpropagation\n",
    "\n",
    "z = y*y*2\n",
    "print(z)  #grad_fn car output, mulbackward car multiplication\n",
    "z = z.mean()   #grad_fn = <MeanBackward>\n",
    "print(z)\n",
    "\n",
    "z.backward()  #dz/dx\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, *requires_grad=True* permet de calculer un Jacobien vectoriel. Pour ce faire, il faut spécifier un vecteur $\\vec{v}$ avec lequelle nous allons calculer le jacobien. Plus haut, nous n'avons pas spécifier de vecteur car nous faisions le backward() d'une valeur (moyenne de z). Si on veux faire le backward() de z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3694, 7.1130, 0.0095])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y*y*2\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "\n",
    "z.backward(v)  #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on veut ne pas calculer le jacobien pour certaines opérations, il est possible de le faire ainsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x.detach()  #Crée un nouveau tenseur y qui ne comptera pas dans le calcul du jacobien\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Insérer calcul ici\n",
    "    y = x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer un gradient dans une boucle for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for _ in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)  # Si on arrête ici, le gradient va se remplir et on va constater que les gradients se sommes\n",
    "\n",
    "    weights.grad.zero_()  #Remet le gradient à 0. On doit le faire après chaque calcul de gradient"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9eaceb9a2ec0e7a70132c062f26382ea71ab72be3751535a08625cae626faf1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
