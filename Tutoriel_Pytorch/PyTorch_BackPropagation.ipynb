{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BackPropagation**\n",
    "\n",
    "Notes and examples taken from the following tutoriel : https://www.youtube.com/watch?v=c36lUUr864M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.)\n",
    "y = torch.tensor(2.)\n",
    "\n",
    "w = torch.tensor(1., requires_grad=True)  #We want gradient of w\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "loss.backward()  #Calculate jacobian and local derivation\n",
    "print(w.grad)  #We find the value of our gradient\n",
    "\n",
    "# For the following:\n",
    "# update weights\n",
    "# next foward and backfoward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent**\n",
    "We begin by calculating the gradient descent manually. In fact, we can seperate the sequence : \n",
    "\n",
    "- Model prediction : Manual\n",
    "- Gradient : Manual\n",
    "- Loss : Manual\n",
    "- Update weights : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training f(5)=1.975\n",
      "epoch 1: w = 1.358, loss = 19.32256699\n",
      "epoch 2: w = 1.743, loss = 3.09161091\n",
      "epoch 3: w = 1.897, loss = 0.49465758\n",
      "epoch 4: w = 1.959, loss = 0.07914524\n",
      "epoch 5: w = 1.984, loss = 0.01266321\n",
      "epoch 6: w = 1.993, loss = 0.00202612\n",
      "epoch 7: w = 1.997, loss = 0.00032418\n",
      "epoch 8: w = 1.999, loss = 0.00005187\n",
      "epoch 9: w = 2.000, loss = 0.00000830\n",
      "epoch 10: w = 2.000, loss = 0.00000133\n",
      "epoch 11: w = 2.000, loss = 0.00000021\n",
      "epoch 12: w = 2.000, loss = 0.00000003\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after training f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x --> (weights * input)\n",
    "# f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)  #training value\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)  # Value of f\n",
    "\n",
    "w = np.random.rand()  # Value of random w\n",
    "\n",
    "# Model prediction\n",
    "\n",
    "def foward(x):  # Foward pass\n",
    "    return w * x\n",
    "\n",
    "# loss = mean square error\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "#gradient\n",
    "# Mean sqaure error = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x-y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f\"Prediction before training f(5)={foward(5):.3f}\")\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction -> foward pass\n",
    "    y_pred = foward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training f(5) = {foward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give a list of input X and a list of output Y, The function that give the relation between X and Y is Y=2*x\n",
    "\n",
    "However, we begin by setting w = 0. We let w converge to 2. We then \"match\" our model.\n",
    "\n",
    "We do the process again by modifying our method:\n",
    "- Prediction : Manual\n",
    "- Gradient : **Autograd**\n",
    "- loss : Manual\n",
    "- update weights : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 2: w = 0.555, loss = 21.67499924\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 4: w = 0.956, loss = 11.31448650\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 6: w = 1.246, loss = 5.90623236\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 8: w = 1.455, loss = 3.08308983\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 10: w = 1.606, loss = 1.60939169\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 12: w = 1.716, loss = 0.84011245\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 14: w = 1.794, loss = 0.43854395\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 16: w = 1.851, loss = 0.22892261\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 18: w = 1.893, loss = 0.11949898\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 20: w = 1.922, loss = 0.06237914\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 22: w = 1.944, loss = 0.03256231\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 24: w = 1.960, loss = 0.01699772\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 26: w = 1.971, loss = 0.00887291\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 28: w = 1.979, loss = 0.00463169\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 30: w = 1.985, loss = 0.00241778\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 32: w = 1.989, loss = 0.00126211\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 34: w = 1.992, loss = 0.00065882\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 36: w = 1.994, loss = 0.00034392\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 38: w = 1.996, loss = 0.00017952\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 40: w = 1.997, loss = 0.00009371\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 42: w = 1.998, loss = 0.00004891\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 44: w = 1.998, loss = 0.00002553\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 46: w = 1.999, loss = 0.00001333\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 48: w = 1.999, loss = 0.00000696\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 50: w = 1.999, loss = 0.00000363\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 52: w = 2.000, loss = 0.00000190\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 54: w = 2.000, loss = 0.00000099\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 56: w = 2.000, loss = 0.00000052\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 58: w = 2.000, loss = 0.00000027\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 60: w = 2.000, loss = 0.00000014\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 62: w = 2.000, loss = 0.00000007\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 64: w = 2.000, loss = 0.00000004\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 66: w = 2.000, loss = 0.00000002\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 68: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 70: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 72: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 74: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 76: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 78: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 80: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 82: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 84: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 88: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 92: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 94: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 98: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "epoch 100: w = 2.000, loss = 0.00000000\n",
      "epoch 101: w = 2.000, loss = 0.00000000\n",
      "epoch 102: w = 2.000, loss = 0.00000000\n",
      "epoch 103: w = 2.000, loss = 0.00000000\n",
      "epoch 104: w = 2.000, loss = 0.00000000\n",
      "epoch 105: w = 2.000, loss = 0.00000000\n",
      "epoch 106: w = 2.000, loss = 0.00000000\n",
      "epoch 107: w = 2.000, loss = 0.00000000\n",
      "epoch 108: w = 2.000, loss = 0.00000000\n",
      "epoch 109: w = 2.000, loss = 0.00000000\n",
      "epoch 110: w = 2.000, loss = 0.00000000\n",
      "epoch 111: w = 2.000, loss = 0.00000000\n",
      "epoch 112: w = 2.000, loss = 0.00000000\n",
      "epoch 113: w = 2.000, loss = 0.00000000\n",
      "epoch 114: w = 2.000, loss = 0.00000000\n",
      "epoch 115: w = 2.000, loss = 0.00000000\n",
      "epoch 116: w = 2.000, loss = 0.00000000\n",
      "epoch 117: w = 2.000, loss = 0.00000000\n",
      "epoch 118: w = 2.000, loss = 0.00000000\n",
      "epoch 119: w = 2.000, loss = 0.00000000\n",
      "epoch 120: w = 2.000, loss = 0.00000000\n",
      "epoch 121: w = 2.000, loss = 0.00000000\n",
      "epoch 122: w = 2.000, loss = 0.00000000\n",
      "epoch 123: w = 2.000, loss = 0.00000000\n",
      "epoch 124: w = 2.000, loss = 0.00000000\n",
      "epoch 125: w = 2.000, loss = 0.00000000\n",
      "epoch 126: w = 2.000, loss = 0.00000000\n",
      "epoch 127: w = 2.000, loss = 0.00000000\n",
      "epoch 128: w = 2.000, loss = 0.00000000\n",
      "epoch 129: w = 2.000, loss = 0.00000000\n",
      "epoch 130: w = 2.000, loss = 0.00000000\n",
      "epoch 131: w = 2.000, loss = 0.00000000\n",
      "epoch 132: w = 2.000, loss = 0.00000000\n",
      "epoch 133: w = 2.000, loss = 0.00000000\n",
      "epoch 134: w = 2.000, loss = 0.00000000\n",
      "epoch 135: w = 2.000, loss = 0.00000000\n",
      "epoch 136: w = 2.000, loss = 0.00000000\n",
      "epoch 137: w = 2.000, loss = 0.00000000\n",
      "epoch 138: w = 2.000, loss = 0.00000000\n",
      "epoch 139: w = 2.000, loss = 0.00000000\n",
      "epoch 140: w = 2.000, loss = 0.00000000\n",
      "epoch 141: w = 2.000, loss = 0.00000000\n",
      "epoch 142: w = 2.000, loss = 0.00000000\n",
      "epoch 143: w = 2.000, loss = 0.00000000\n",
      "epoch 144: w = 2.000, loss = 0.00000000\n",
      "epoch 145: w = 2.000, loss = 0.00000000\n",
      "epoch 146: w = 2.000, loss = 0.00000000\n",
      "epoch 147: w = 2.000, loss = 0.00000000\n",
      "epoch 148: w = 2.000, loss = 0.00000000\n",
      "epoch 149: w = 2.000, loss = 0.00000000\n",
      "epoch 150: w = 2.000, loss = 0.00000000\n",
      "epoch 151: w = 2.000, loss = 0.00000000\n",
      "epoch 152: w = 2.000, loss = 0.00000000\n",
      "epoch 153: w = 2.000, loss = 0.00000000\n",
      "epoch 154: w = 2.000, loss = 0.00000000\n",
      "epoch 155: w = 2.000, loss = 0.00000000\n",
      "epoch 156: w = 2.000, loss = 0.00000000\n",
      "epoch 157: w = 2.000, loss = 0.00000000\n",
      "epoch 158: w = 2.000, loss = 0.00000000\n",
      "epoch 159: w = 2.000, loss = 0.00000000\n",
      "epoch 160: w = 2.000, loss = 0.00000000\n",
      "epoch 161: w = 2.000, loss = 0.00000000\n",
      "epoch 162: w = 2.000, loss = 0.00000000\n",
      "epoch 163: w = 2.000, loss = 0.00000000\n",
      "epoch 164: w = 2.000, loss = 0.00000000\n",
      "epoch 165: w = 2.000, loss = 0.00000000\n",
      "epoch 166: w = 2.000, loss = 0.00000000\n",
      "epoch 167: w = 2.000, loss = 0.00000000\n",
      "epoch 168: w = 2.000, loss = 0.00000000\n",
      "epoch 169: w = 2.000, loss = 0.00000000\n",
      "epoch 170: w = 2.000, loss = 0.00000000\n",
      "epoch 171: w = 2.000, loss = 0.00000000\n",
      "epoch 172: w = 2.000, loss = 0.00000000\n",
      "epoch 173: w = 2.000, loss = 0.00000000\n",
      "epoch 174: w = 2.000, loss = 0.00000000\n",
      "epoch 175: w = 2.000, loss = 0.00000000\n",
      "epoch 176: w = 2.000, loss = 0.00000000\n",
      "epoch 177: w = 2.000, loss = 0.00000000\n",
      "epoch 178: w = 2.000, loss = 0.00000000\n",
      "epoch 179: w = 2.000, loss = 0.00000000\n",
      "epoch 180: w = 2.000, loss = 0.00000000\n",
      "epoch 181: w = 2.000, loss = 0.00000000\n",
      "epoch 182: w = 2.000, loss = 0.00000000\n",
      "epoch 183: w = 2.000, loss = 0.00000000\n",
      "epoch 184: w = 2.000, loss = 0.00000000\n",
      "epoch 185: w = 2.000, loss = 0.00000000\n",
      "epoch 186: w = 2.000, loss = 0.00000000\n",
      "epoch 187: w = 2.000, loss = 0.00000000\n",
      "epoch 188: w = 2.000, loss = 0.00000000\n",
      "epoch 189: w = 2.000, loss = 0.00000000\n",
      "epoch 190: w = 2.000, loss = 0.00000000\n",
      "epoch 191: w = 2.000, loss = 0.00000000\n",
      "epoch 192: w = 2.000, loss = 0.00000000\n",
      "epoch 193: w = 2.000, loss = 0.00000000\n",
      "epoch 194: w = 2.000, loss = 0.00000000\n",
      "epoch 195: w = 2.000, loss = 0.00000000\n",
      "epoch 196: w = 2.000, loss = 0.00000000\n",
      "epoch 197: w = 2.000, loss = 0.00000000\n",
      "epoch 198: w = 2.000, loss = 0.00000000\n",
      "epoch 199: w = 2.000, loss = 0.00000000\n",
      "epoch 200: w = 2.000, loss = 0.00000000\n",
      "Prediction after training f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x --> (weights * input)\n",
    "# f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)  #training value\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)  # value of f\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) # random value of w -> this value will converge to 2.\n",
    "\n",
    "# Model prediction\n",
    "\n",
    "def foward(x):  # Foward pass\n",
    "    return w * x\n",
    "\n",
    "# loss = mean square error\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 200\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction -> foward pass\n",
    "    y_pred = foward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad  #Not part of the jacobian calculation\n",
    "\n",
    "    w.grad.zero_()  # Reset gradient to 0.\n",
    "    \n",
    "\n",
    "    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training f(5) = {foward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By using BackPropagation directly with PyTorch, we have to use more iterations to converge to the correct model. It is due to the fact that BackPropagation is not as precise as a numerical aproach\n",
    "\n",
    "*Remark : With $w -= \\text{leanrning rate} \\times dw$ we go in the negative direction of the negative direction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Pipeline**\n",
    "We now use PyTorch to calculate the loss and the weight's update. The sequence is now :\n",
    "\n",
    "- Prediction : Manual\n",
    "- Gradient : **Autograd**\n",
    "- Loss : **PyTorch Loss**\n",
    "- Update of the weights : **PyTorch Optimizer**\n",
    "\n",
    "We begin by :\n",
    "* 1) desin of the model (input, output size, foward)\n",
    "* 2) Build the loss and the optimizer\n",
    "* 3) Loop training : \n",
    "\n",
    "Foward pass $\\rightarrow$ prediction\n",
    "\n",
    "Backward pass $\\rightarrow$ gradient\n",
    "\n",
    "Update of the weight $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training f(5)=60.000\n",
      "epoch 1: w = 10.500, loss = 750.00000000\n",
      "epoch 2: w = 9.225, loss = 541.87500000\n",
      "epoch 3: w = 8.141, loss = 391.50473022\n",
      "epoch 4: w = 7.220, loss = 282.86218262\n",
      "epoch 5: w = 6.437, loss = 204.36795044\n",
      "epoch 6: w = 5.771, loss = 147.65583801\n",
      "epoch 7: w = 5.206, loss = 106.68135071\n",
      "epoch 8: w = 4.725, loss = 77.07728577\n",
      "epoch 9: w = 4.316, loss = 55.68834686\n",
      "epoch 10: w = 3.969, loss = 40.23483276\n",
      "epoch 11: w = 3.673, loss = 29.06967163\n",
      "epoch 12: w = 3.422, loss = 21.00283623\n",
      "epoch 13: w = 3.209, loss = 15.17454815\n",
      "epoch 14: w = 3.028, loss = 10.96361351\n",
      "epoch 15: w = 2.874, loss = 7.92121029\n",
      "epoch 16: w = 2.743, loss = 5.72307396\n",
      "epoch 17: w = 2.631, loss = 4.13492203\n",
      "epoch 18: w = 2.536, loss = 2.98748064\n",
      "epoch 19: w = 2.456, loss = 2.15845394\n",
      "epoch 20: w = 2.388, loss = 1.55948305\n",
      "epoch 21: w = 2.329, loss = 1.12672663\n",
      "epoch 22: w = 2.280, loss = 0.81405973\n",
      "epoch 23: w = 2.238, loss = 0.58815801\n",
      "epoch 24: w = 2.202, loss = 0.42494452\n",
      "epoch 25: w = 2.172, loss = 0.30702239\n",
      "epoch 26: w = 2.146, loss = 0.22182392\n",
      "epoch 27: w = 2.124, loss = 0.16026792\n",
      "epoch 28: w = 2.106, loss = 0.11579335\n",
      "epoch 29: w = 2.090, loss = 0.08366069\n",
      "epoch 30: w = 2.076, loss = 0.06044482\n",
      "epoch 31: w = 2.065, loss = 0.04367133\n",
      "epoch 32: w = 2.055, loss = 0.03155241\n",
      "epoch 33: w = 2.047, loss = 0.02279668\n",
      "epoch 34: w = 2.040, loss = 0.01647059\n",
      "epoch 35: w = 2.034, loss = 0.01190005\n",
      "epoch 36: w = 2.029, loss = 0.00859775\n",
      "epoch 37: w = 2.024, loss = 0.00621186\n",
      "epoch 38: w = 2.021, loss = 0.00448808\n",
      "epoch 39: w = 2.018, loss = 0.00324269\n",
      "epoch 40: w = 2.015, loss = 0.00234283\n",
      "epoch 41: w = 2.013, loss = 0.00169267\n",
      "epoch 42: w = 2.011, loss = 0.00122294\n",
      "epoch 43: w = 2.009, loss = 0.00088357\n",
      "epoch 44: w = 2.008, loss = 0.00063837\n",
      "epoch 45: w = 2.007, loss = 0.00046123\n",
      "epoch 46: w = 2.006, loss = 0.00033324\n",
      "epoch 47: w = 2.005, loss = 0.00024076\n",
      "epoch 48: w = 2.004, loss = 0.00017394\n",
      "epoch 49: w = 2.003, loss = 0.00012567\n",
      "epoch 50: w = 2.003, loss = 0.00009080\n",
      "epoch 51: w = 2.003, loss = 0.00006561\n",
      "epoch 52: w = 2.002, loss = 0.00004740\n",
      "epoch 53: w = 2.002, loss = 0.00003424\n",
      "epoch 54: w = 2.002, loss = 0.00002474\n",
      "epoch 55: w = 2.001, loss = 0.00001787\n",
      "epoch 56: w = 2.001, loss = 0.00001292\n",
      "epoch 57: w = 2.001, loss = 0.00000933\n",
      "epoch 58: w = 2.001, loss = 0.00000674\n",
      "epoch 59: w = 2.001, loss = 0.00000487\n",
      "epoch 60: w = 2.001, loss = 0.00000352\n",
      "epoch 61: w = 2.000, loss = 0.00000254\n",
      "epoch 62: w = 2.000, loss = 0.00000184\n",
      "epoch 63: w = 2.000, loss = 0.00000133\n",
      "epoch 64: w = 2.000, loss = 0.00000096\n",
      "epoch 65: w = 2.000, loss = 0.00000069\n",
      "epoch 66: w = 2.000, loss = 0.00000050\n",
      "epoch 67: w = 2.000, loss = 0.00000036\n",
      "epoch 68: w = 2.000, loss = 0.00000026\n",
      "epoch 69: w = 2.000, loss = 0.00000019\n",
      "epoch 70: w = 2.000, loss = 0.00000014\n",
      "epoch 71: w = 2.000, loss = 0.00000010\n",
      "epoch 72: w = 2.000, loss = 0.00000007\n",
      "epoch 73: w = 2.000, loss = 0.00000005\n",
      "epoch 74: w = 2.000, loss = 0.00000004\n",
      "epoch 75: w = 2.000, loss = 0.00000003\n",
      "epoch 76: w = 2.000, loss = 0.00000002\n",
      "epoch 77: w = 2.000, loss = 0.00000001\n",
      "epoch 78: w = 2.000, loss = 0.00000001\n",
      "epoch 79: w = 2.000, loss = 0.00000001\n",
      "epoch 80: w = 2.000, loss = 0.00000001\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 82: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 84: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 88: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 92: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 94: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 98: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "epoch 100: w = 2.000, loss = 0.00000000\n",
      "Prediction after training f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)  #training value\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)  # value of f\n",
    "\n",
    "w = torch.tensor(12.0, dtype=torch.float32, requires_grad=True) # random w -> Value will be adapt\n",
    "\n",
    "# Model prediction\n",
    "def foward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f\"Prediction before training f(5)={foward(5):.3f}\")\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()  # We take the function directly from PyTorch\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)  #SGD -> Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction -> foward pass\n",
    "    y_pred = foward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad() # Gradient back to 0 after each iterations\n",
    "\n",
    "    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training f(5) = {foward(5):.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to do our entire gradient descent with PyTorch\n",
    "\n",
    "- Prediction : **PyTorch Model**\n",
    "- Gradient : **Autograd**\n",
    "- Loss : **PyTorch Loss**\n",
    "- Update of the weight : **PyTorch Optimizer**\n",
    "\n",
    "We have to convert our samples in 2D array where the number of lines is equal to the number of samples we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training f(5)=-3.075\n",
      "epoch 1: w = 3.645, loss = 55.91775131\n",
      "epoch 11: w = 2.033, loss = 0.01862619\n",
      "epoch 21: w = 2.003, loss = 0.00001289\n",
      "epoch 31: w = 2.002, loss = 0.00000365\n",
      "epoch 41: w = 2.001, loss = 0.00000198\n",
      "epoch 51: w = 2.001, loss = 0.00000108\n",
      "epoch 61: w = 2.001, loss = 0.00000059\n",
      "epoch 71: w = 2.000, loss = 0.00000032\n",
      "epoch 81: w = 2.000, loss = 0.00000017\n",
      "epoch 91: w = 2.000, loss = 0.00000009\n",
      "epoch 101: w = 2.000, loss = 0.00000005\n",
      "epoch 111: w = 2.000, loss = 0.00000003\n",
      "epoch 121: w = 2.000, loss = 0.00000002\n",
      "epoch 131: w = 2.000, loss = 0.00000001\n",
      "epoch 141: w = 2.000, loss = 0.00000000\n",
      "epoch 151: w = 2.000, loss = 0.00000000\n",
      "epoch 161: w = 2.000, loss = 0.00000000\n",
      "epoch 171: w = 2.000, loss = 0.00000000\n",
      "epoch 181: w = 2.000, loss = 0.00000000\n",
      "epoch 191: w = 2.000, loss = 0.00000000\n",
      "epoch 201: w = 2.000, loss = 0.00000000\n",
      "epoch 211: w = 2.000, loss = 0.00000000\n",
      "epoch 221: w = 2.000, loss = 0.00000000\n",
      "epoch 231: w = 2.000, loss = 0.00000000\n",
      "epoch 241: w = 2.000, loss = 0.00000000\n",
      "epoch 251: w = 2.000, loss = 0.00000000\n",
      "epoch 261: w = 2.000, loss = 0.00000000\n",
      "epoch 271: w = 2.000, loss = 0.00000000\n",
      "epoch 281: w = 2.000, loss = 0.00000000\n",
      "epoch 291: w = 2.000, loss = 0.00000000\n",
      "epoch 301: w = 2.000, loss = 0.00000000\n",
      "epoch 311: w = 2.000, loss = 0.00000000\n",
      "epoch 321: w = 2.000, loss = 0.00000000\n",
      "epoch 331: w = 2.000, loss = 0.00000000\n",
      "epoch 341: w = 2.000, loss = 0.00000000\n",
      "epoch 351: w = 2.000, loss = 0.00000000\n",
      "epoch 361: w = 2.000, loss = 0.00000000\n",
      "epoch 371: w = 2.000, loss = 0.00000000\n",
      "epoch 381: w = 2.000, loss = 0.00000000\n",
      "epoch 391: w = 2.000, loss = 0.00000000\n",
      "epoch 401: w = 2.000, loss = 0.00000000\n",
      "epoch 411: w = 2.000, loss = 0.00000000\n",
      "epoch 421: w = 2.000, loss = 0.00000000\n",
      "epoch 431: w = 2.000, loss = 0.00000000\n",
      "epoch 441: w = 2.000, loss = 0.00000000\n",
      "epoch 451: w = 2.000, loss = 0.00000000\n",
      "epoch 461: w = 2.000, loss = 0.00000000\n",
      "epoch 471: w = 2.000, loss = 0.00000000\n",
      "epoch 481: w = 2.000, loss = 0.00000000\n",
      "epoch 491: w = 2.000, loss = 0.00000000\n",
      "Prediction after training f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  #training value\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)  # value of f\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) # Random w\n",
    "\n",
    "# Model prediction\n",
    "n_samples, n_feature = X.shape\n",
    "print(n_samples, n_feature)\n",
    "\n",
    "\n",
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f\"Prediction before training f(5)={model(X_test).item():.3f}\")\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.1\n",
    "n_iters = 500\n",
    "\n",
    "loss = nn.MSELoss()  # Function directly in PyTorch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  #SGD -> Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction -> foward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad() # Reset gradient to 0\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()  #Return parameter : [[w]]\n",
    "        print(f\"epoch {epoch+1}: w = {w[0][0]:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training f(5) = {model(X_test).item():.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we used models that were already in PyTorch. We can however create a new model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # We define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def foward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)  #  = nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression with PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10, loss = 4473.7314\n",
      "epoch : 20, loss = 3333.8506\n",
      "epoch : 30, loss = 2509.7849\n",
      "epoch : 40, loss = 1913.3629\n",
      "epoch : 50, loss = 1481.2472\n",
      "epoch : 60, loss = 1167.8705\n",
      "epoch : 70, loss = 940.4011\n",
      "epoch : 80, loss = 775.1529\n",
      "epoch : 90, loss = 655.0142\n",
      "epoch : 100, loss = 567.6100\n",
      "epoch : 110, loss = 503.9806\n",
      "epoch : 120, loss = 457.6313\n",
      "epoch : 130, loss = 423.8513\n",
      "epoch : 140, loss = 399.2195\n",
      "epoch : 150, loss = 381.2507\n",
      "epoch : 160, loss = 368.1368\n",
      "epoch : 170, loss = 358.5627\n",
      "epoch : 180, loss = 351.5704\n",
      "epoch : 190, loss = 346.4621\n",
      "epoch : 200, loss = 342.7292\n",
      "epoch : 210, loss = 340.0005\n",
      "epoch : 220, loss = 338.0055\n",
      "epoch : 230, loss = 336.5466\n",
      "epoch : 240, loss = 335.4795\n",
      "epoch : 250, loss = 334.6988\n",
      "epoch : 260, loss = 334.1276\n",
      "epoch : 270, loss = 333.7096\n",
      "epoch : 280, loss = 333.4037\n",
      "epoch : 290, loss = 333.1798\n",
      "epoch : 300, loss = 333.0159\n",
      "epoch : 310, loss = 332.8959\n",
      "epoch : 320, loss = 332.8080\n",
      "epoch : 330, loss = 332.7437\n",
      "epoch : 340, loss = 332.6966\n",
      "epoch : 350, loss = 332.6621\n",
      "epoch : 360, loss = 332.6368\n",
      "epoch : 370, loss = 332.6183\n",
      "epoch : 380, loss = 332.6047\n",
      "epoch : 390, loss = 332.5948\n",
      "epoch : 400, loss = 332.5875\n",
      "epoch : 410, loss = 332.5822\n",
      "epoch : 420, loss = 332.5783\n",
      "epoch : 430, loss = 332.5754\n",
      "epoch : 440, loss = 332.5733\n",
      "epoch : 450, loss = 332.5717\n",
      "epoch : 460, loss = 332.5707\n",
      "epoch : 470, loss = 332.5699\n",
      "epoch : 480, loss = 332.5692\n",
      "epoch : 490, loss = 332.5688\n",
      "epoch : 500, loss = 332.5685\n",
      "epoch : 510, loss = 332.5682\n",
      "epoch : 520, loss = 332.5681\n",
      "epoch : 530, loss = 332.5679\n",
      "epoch : 540, loss = 332.5678\n",
      "epoch : 550, loss = 332.5677\n",
      "epoch : 560, loss = 332.5677\n",
      "epoch : 570, loss = 332.5677\n",
      "epoch : 580, loss = 332.5677\n",
      "epoch : 590, loss = 332.5676\n",
      "epoch : 600, loss = 332.5676\n",
      "epoch : 610, loss = 332.5676\n",
      "epoch : 620, loss = 332.5676\n",
      "epoch : 630, loss = 332.5676\n",
      "epoch : 640, loss = 332.5676\n",
      "epoch : 650, loss = 332.5676\n",
      "epoch : 660, loss = 332.5676\n",
      "epoch : 670, loss = 332.5676\n",
      "epoch : 680, loss = 332.5675\n",
      "epoch : 690, loss = 332.5675\n",
      "epoch : 700, loss = 332.5675\n",
      "epoch : 710, loss = 332.5676\n",
      "epoch : 720, loss = 332.5676\n",
      "epoch : 730, loss = 332.5676\n",
      "epoch : 740, loss = 332.5676\n",
      "epoch : 750, loss = 332.5676\n",
      "epoch : 760, loss = 332.5676\n",
      "epoch : 770, loss = 332.5676\n",
      "epoch : 780, loss = 332.5675\n",
      "epoch : 790, loss = 332.5676\n",
      "epoch : 800, loss = 332.5676\n",
      "epoch : 810, loss = 332.5676\n",
      "epoch : 820, loss = 332.5676\n",
      "epoch : 830, loss = 332.5676\n",
      "epoch : 840, loss = 332.5676\n",
      "epoch : 850, loss = 332.5676\n",
      "epoch : 860, loss = 332.5676\n",
      "epoch : 870, loss = 332.5676\n",
      "epoch : 880, loss = 332.5676\n",
      "epoch : 890, loss = 332.5676\n",
      "epoch : 900, loss = 332.5676\n",
      "epoch : 910, loss = 332.5676\n",
      "epoch : 920, loss = 332.5676\n",
      "epoch : 930, loss = 332.5676\n",
      "epoch : 940, loss = 332.5676\n",
      "epoch : 950, loss = 332.5676\n",
      "epoch : 960, loss = 332.5676\n",
      "epoch : 970, loss = 332.5676\n",
      "epoch : 980, loss = 332.5676\n",
      "epoch : 990, loss = 332.5676\n",
      "epoch : 1000, loss = 332.5676\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  #nn -> neural network\n",
    "import numpy as np\n",
    "from sklearn import datasets  # for samples\n",
    "import matplotlib_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "# noise = The standard deviation of the gaussian noise applied to the output.\n",
    "# random_state = Determines random number generation for dataset creation\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "\n",
    "# We want to convert Y in column vector\n",
    "\n",
    "Y = Y.view(Y.shape[0], 1)\n",
    "\n",
    "n_samples, n_feature = X.shape\n",
    "\n",
    "# 1) Model\n",
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) Loss and Optimizer\n",
    "learning_rate = 0.01\n",
    "loss = nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training Loop\n",
    "n_iters = 1000\n",
    "for epoch in range(n_iters):\n",
    "    # foward pass\n",
    "    y_predicted = model(X)\n",
    "    l = loss(y_predicted, Y)\n",
    "\n",
    "    # backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()  #update weight\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch : {epoch+1}, loss = {l.item():.4f}\")\n",
    "\n",
    "\n",
    "#plot\n",
    "predicted = model(X).detach().numpy()  # Convert tensor -> np.array. No more gradient calculation.\n",
    "\n",
    "plt.plot(X_numpy, Y_numpy, \"ro\")\n",
    "plt.plot(X_numpy, predicted, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logisitic Regression**\n",
    "\n",
    "A logistic regression is use when dealing with binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.6292\n",
      "epoch: 20, loss = 0.5064\n",
      "epoch: 30, loss = 0.4319\n",
      "epoch: 40, loss = 0.3820\n",
      "epoch: 50, loss = 0.3460\n",
      "epoch: 60, loss = 0.3186\n",
      "epoch: 70, loss = 0.2970\n",
      "epoch: 80, loss = 0.2793\n",
      "epoch: 90, loss = 0.2646\n",
      "epoch: 100, loss = 0.2520\n",
      "accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)  #mean 0 with variance\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)  # y in column vector\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1) Model\n",
    "# Linear model at first then sigmoid at the extremes\n",
    "class Logistic(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Logistic, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)  #We want 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = Logistic(n_features)\n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()  #Binary cross entropy loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()  #Update weights\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()  #If < 0.5 -> 0 else 1\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])  #If y_pred = Y_test -> 1. We do the mean if these 1\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9eaceb9a2ec0e7a70132c062f26382ea71ab72be3751535a08625cae626faf1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
